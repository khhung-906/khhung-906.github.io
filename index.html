<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hung-Chieh (Oscar) Fang</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hung-Chieh (Oscar) Fang
                </p>
                <p>I am a senior undergraduate majoring in Computer Science at National Taiwan University, where I am fortunate to be advised by Professors <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>, <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a> and <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php">Hung-yi Lee</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:b09902106@csie.ntu.edu.tw">Email</a> &nbsp;/&nbsp;
                  <a href="data/cv_hcfang.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MpGIrR0AAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/hc-fang/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/HUNG-CHIEH-FANG/">Linkedin</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/hcfang.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/hcfang.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am broadly interested in robust machine learning and robotics. Specifically, I aim to explore how to enable robots to learn and adapt in dynamic environments.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/unida.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2410.11271">
                  <span class="papertitle">Reducing Source-Private Bias in Extreme Universal Domain Adaptation
                  </span>
                </a>
                <br>
                <strong>Hung-Chieh Fang</strong>, <a href="https://openreview.net/profile?id=~Po-Yi_Lu1">Po-Yi Lu</a>, <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>
                <br>
                <em>Preprint. Under Review.</em>
                <br>
                <a href="https://arxiv.org/abs/2410.11271">paper</a>
                <p></p>
                <p>
                  We identify the unsolved Extreme UniDA sub-task, highlighting the limitations of existing partial domain alignment paradigms. We propose using self-supervised loss to reduce source-private bias and improve robustness across scenarios.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/pwhubert.png' width="170">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2402.05819">
                  <span class="papertitle">Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model
                  </span>
                </a>
                <br>
                <strong>Hung-Chieh Fang*</strong>, Nai-Xuan Ye*, <a href="https://www.cs.utexas.edu/~yjshih/">Yi-Jen Shih</a>, <a href="https://jasonppy.github.io/">Puyuan Peng</a>, Hsuan-Fu Wang, Layne Berry, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php">Hung-yi Lee</a>, <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>
                <br>
                <em>ICASSP 2024 workshop: Self-supervision in Audio, Speech and Beyond </em>
                <br>
                <a href="https://arxiv.org/abs/2402.05819">paper</a>
                <p></p>
                <p>
                  We propose using vision as a surrogate for paired transcripts to enrich the semantic information in self-supervised speech models.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/convadrqa.png' width="170">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2211.09401">
                  <span class="papertitle">Open-domain Conversational Question Answering with Historical Answers</span>
                </a>
                <br>
                <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung906.github.io/">Kuo-Han Hung</a>*, <a href="https://chaoweihuang.github.io/">Chao-Wei Huang</a>, <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung Chen</a>
                <br>
                <em>AACL-IJCNLP 2022</em>
                <br>
                <a href="https://arxiv.org/abs/2211.09401">paper</a> / <a href="https://github.com/MiuLab/ConvADR-QA">code</a>
                <p></p>
                <p>
                  We propose combining the signal from historical answers with the noise-reduction ability of knowledge distillation to improve information retrieval and question answering.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Projects</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/text_br.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hc-fang.github.io/data/Text_Behavior_Retrieval.pdf">
                <span class="papertitle">Zero-shot Text Behavior Retrieval
                </span>
              </a>
              <br>
              <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung906.github.io/">Kuo-Han Hung</a>*,  Nai-Xuan Ye*, Shao-Syuan Huang*
              <br>
              <em>Course Project of "Reinforcement Learning", Spring 2023</em>
              <br>
              <a href="data/Text_Behavior_Retrieval.pdf">paper</a>
              <p></p>
              <p>
                We propose a method for retrieving task-relevant data for imitation learning without requiring expert demonstrations. Our approach leverages text descriptions in combination with a vision-language model to enable zero-shot behavior retrieval.
              </p>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Teaching</h2>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/ntu_logo.png' width="140">
            </div>
          </td>
          <td width="75%" valign="center">
            Teaching Assistant, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php" >EE5100: Introduction to Generative Artificial Intelligence, Summer 2024</a>
            <br>
            <br>
            Teaching Assistant, <a href="https://www.csie.ntu.edu.tw/~htlin/course/ml23spring/" >CSIE5043: Machine Learning, Summer 2023</a>
          </td>
        </tr>

      </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This template is adapted from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
